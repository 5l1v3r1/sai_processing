{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nнайти дескрипторы\\nи\\nсовместить\\n\\nhttps://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/\\n\\nhttps://www.learnopencv.com/homography-examples-using-opencv-python-c/\\n\\nhttps://github.com/vivekseth/blog-posts/tree/master/Difference-between-perspective-transform-homography-essential-matrix-fundamental-matrix\\n\\nopencv getaffinetransform homography\\n\\nopencv homography panorama python\\n\\nhttps://stackoverflow.com/questions/11237948/findhomography-getperspectivetransform-getaffinetransform\\nhttps://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=getaffinetransform\\n\\nhttps://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#Mat%20findHomography(InputArray%20srcPoints,%20InputArray%20dstPoints,%20int%20method,%20double%20ransacReprojThreshold,%20OutputArray%20mask)\\n\\nhttp://answers.opencv.org/question/4538/getaffinetransform-getperspectivetransform-or-findhomography/\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "'''\n",
    "найти дескрипторы\n",
    "и\n",
    "совместить\n",
    "\n",
    "https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/\n",
    "\n",
    "https://www.learnopencv.com/homography-examples-using-opencv-python-c/\n",
    "\n",
    "https://github.com/vivekseth/blog-posts/tree/master/Difference-between-perspective-transform-homography-essential-matrix-fundamental-matrix\n",
    "\n",
    "opencv getaffinetransform homography\n",
    "\n",
    "opencv homography panorama python\n",
    "\n",
    "https://stackoverflow.com/questions/11237948/findhomography-getperspectivetransform-getaffinetransform\n",
    "https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=getaffinetransform\n",
    "\n",
    "https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#Mat%20findHomography(InputArray%20srcPoints,%20InputArray%20dstPoints,%20int%20method,%20double%20ransacReprojThreshold,%20OutputArray%20mask)\n",
    "\n",
    "http://answers.opencv.org/question/4538/getaffinetransform-getperspectivetransform-or-findhomography/\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = ['None','gauss', 's&p', 'poisson', 'speckle']\n",
    "\n",
    "def noisy(noise_typ,image):\n",
    "    if noise_typ == \"gauss\":\n",
    "        row,col,ch= image.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        noisy = image + gauss\n",
    "        return noisy\n",
    "    elif noise_typ == \"s&p\":\n",
    "        row,col,ch = image.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "        out[tuple(coords)] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "        out[tuple(coords)] = 0\n",
    "        return out\n",
    "    elif noise_typ == \"poisson\":\n",
    "        vals = len(np.unique(image))\n",
    "        vals = 2 ** np.ceil(np.log2(vals))\n",
    "        noisy = np.random.poisson(image * vals) / float(vals)\n",
    "        return noisy\n",
    "    elif noise_typ ==\"speckle\":\n",
    "        row,col,ch = image.shape\n",
    "        gauss = np.random.randn(row,col,ch)\n",
    "        gauss = gauss.reshape(row,col,ch)        \n",
    "        noisy = image + image * gauss\n",
    "        return noisy\n",
    "    \n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher BRUTEFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0183be56704da685d25c1c0f5873a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='noise', options=('None', 'gauss', 's&p', 'poisson', 'speckle'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(noise=noises, rotate=(0, 360), scale=(0.5, 1.5, 1e-2),\n",
    "          minHessian=(1, 50000, 1))\n",
    "def features_1_detect(noise, rotate, scale,\n",
    "                   minHessian):\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    img1 = cv.imread('/Users/kolsha/Desktop/milk1.jpg')\n",
    "    img2 = cv.imread('/Users/kolsha/Desktop/milk2.jpg')\n",
    "\n",
    "    img1 = cv.cvtColor(img1, cv.COLOR_BGR2RGB)\n",
    "    img2 = cv.cvtColor(img2, cv.COLOR_BGR2RGB)\n",
    "    img1_gray = cv.cvtColor(img1, cv.COLOR_RGB2GRAY)\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_rows, num_cols = img1_gray.shape[:2]\n",
    "\n",
    "    rotation_matrix = cv.getRotationMatrix2D((num_cols/2, num_rows/2), rotate, scale)\n",
    "    \n",
    "    \n",
    "    img2 = noisy(noise, img2)\n",
    "    img2 = img2.astype('uint8')\n",
    "    \n",
    "    \n",
    "    img2 = cv.warpAffine(img2, rotation_matrix, (num_cols, num_rows))\n",
    "    \n",
    "    img2_gray = cv.cvtColor(img2, cv.COLOR_RGB2GRAY)\n",
    "    \n",
    "    #-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n",
    "    detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "    \n",
    "    #detector = cv.ORB_create(100)\n",
    "    \n",
    "    keypoints1, descriptors1 = detector.detectAndCompute(img1_gray, None)\n",
    "    keypoints2, descriptors2 = detector.detectAndCompute(img2_gray, None)\n",
    "    #-- Step 2: Matching descriptor vectors with a brute force matcher\n",
    "    # Since SURF is a floating-point descriptor NORM_L2 is used\n",
    "    \n",
    "    #matcher = cv.DescriptorMatcher_create(cv.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    #matches = matcher.match(descriptors1, descriptors2)\n",
    "    matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\n",
    "    matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n",
    "    #matcher.knnMatch()\n",
    "    \n",
    "    #matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "    \n",
    "    #numGoodMatches = int(len(matches) * 0.7)\n",
    "    #matches = matches[:numGoodMatches]\n",
    "    \n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "            \n",
    "    matches = good_matches\n",
    "    \n",
    "    #-- Draw matches\n",
    "    img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\n",
    "    cv.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)\n",
    "    \n",
    "    \n",
    "      # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "        \n",
    "        \n",
    "    h, mask = cv.findHomography(points1, points2, cv.RANSAC) # устойчив \n",
    "    \n",
    "    height, width, channels = img2.shape\n",
    "    img1Reg = cv.warpPerspective(img1, h, (width, height))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    titles = ['img_matches', 'warped']#['Original Image', 'Tpl', 'C Image', 'C Tpl']\n",
    "    images = [img_matches,\n",
    "             np.concatenate((img1Reg, img2), axis=1)\n",
    "             ]#[src, tpl, dst_src, dst_tpl]\n",
    "    #fig = plt.gcf()\n",
    "    #fig.set_size_inches(20, 20)\n",
    "    for i in range(len(images)):\n",
    "        #plt.subplot(2, 3, i+1)\n",
    "        plt.figure(figsize = (20,10))\n",
    "        plt.imshow(images[i], aspect='auto')\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks([]),\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    return (rotate, scale)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher_FLANNBASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472123ae5e6143d49252c250bf9f00b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='noise', options=('None', 'gauss', 's&p', 'poisson', 'speckle'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(noise=noises, rotate=(0, 360), scale=(0.5, 1.5, 1e-2),\n",
    "          minHessian=(1, 50000, 1), ratio_thresh=(0.1, 2.0, 0.1))\n",
    "def features_2_detect(noise, rotate, scale,\n",
    "                   minHessian, ratio_thresh):\n",
    "    \n",
    "    \n",
    "    \n",
    "    src = cv.imread('grabCut/bike.jpg')\n",
    "    #src_gray = cv.medianBlur(src_gray, 5)\n",
    "    src = cv.cvtColor(src, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    src_gray = cv.cvtColor(src, cv.COLOR_RGB2GRAY)\n",
    "    \n",
    "    img = src.copy()\n",
    "    num_rows, num_cols = img.shape[:2]\n",
    "\n",
    "    rotation_matrix = cv.getRotationMatrix2D((num_cols/2, num_rows/2), rotate, scale)\n",
    "    \n",
    "    \n",
    "    img = noisy(noise, img)\n",
    "    img = img.astype('uint8')\n",
    "    \n",
    "    \n",
    "    img = cv.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
    "    \n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    \n",
    "    detector = cv.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "    #sift = cv.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    keypoints1, descriptors1 = detector.detectAndCompute(src_gray, None)\n",
    "    keypoints2, descriptors2 = detector.detectAndCompute(img_gray, None)\n",
    "    \n",
    "    #-- Step 2: Matching descriptor vectors with a FLANN based matcher\n",
    "    # Since SURF is a floating-point descriptor NORM_L2 is used\n",
    "    matcher = cv.DescriptorMatcher_create(cv.DescriptorMatcher_FLANNBASED)\n",
    "    knn_matches = matcher.knnMatch(descriptors1, descriptors2, 2)\n",
    "    #-- Filter matches using the Lowe's ratio test\n",
    "    \n",
    "    good_matches = []\n",
    "    for m,n in knn_matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good_matches.append(m)\n",
    "    #-- Draw matches\n",
    "    img_matches = np.empty((max(src.shape[0], img.shape[0]), src.shape[1]+img.shape[1], 3), dtype=np.uint8)\n",
    "    cv.drawMatches(src, keypoints1, img, keypoints2, good_matches, img_matches, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    titles = ['img_matches']#['Original Image', 'Tpl', 'C Image', 'C Tpl']\n",
    "    images = [img_matches]#[src, tpl, dst_src, dst_tpl]\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    for i in range(len(images)):\n",
    "        #plt.subplot(2, 3, i+1)\n",
    "        plt.figure(figsize = (20,10))\n",
    "        plt.imshow(images[i], aspect='auto')\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks([]),\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    return (rotate, scale)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
